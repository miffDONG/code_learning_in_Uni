{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e0a3c27",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "1bef086d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hyundong\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hyundong\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\hyundong\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 필요한 library가 있다면 추가하셔도 됩니다.\n",
    "\n",
    "# load all necessary libraries\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "%matplotlib inline\n",
    "\n",
    "# libraries for nlp task\n",
    "import regex as re\n",
    "import nltk, re, string\n",
    "from nltk import FreqDist\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(42)\n",
    "# filtering warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from nltk.corpus import wordnet, stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119b8238",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "data의 preprocessing을 진행합니다. 아래 조건에 맞게 전처리를 진행합니다. 필요한만큼 셀을 사용하시면 됩니다.\n",
    "\n",
    "조건에 맞는 전처리를 진행하고 각각의 실행 결과(output) 창을 보여주어야합니다.\n",
    "\n",
    "- Sentence의 문자를 모두 소문자로 변경\n",
    "- stopwords의 english stopwords 제거\n",
    "- WordNetLemmatizer를 이용하여 lemmatize 진행\n",
    "- 정규 표현식을 사용하여 url 제거\n",
    "- 정규 표현식을 사용하여 알파벳을 제외한 punctuation 포함 문자 제거\n",
    "- sklearn의 LabelEncoder를 이용하여 Sentiment에 대해 label encoding 진행\n",
    "- FreqDist를 사용하여 word encoding\n",
    "- 제일 길이가 긴 문장을 기준으로 zero-padding 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b908d1c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The GeoSolutions technology will leverage Bene...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$ESI on lows, down $1.50 to $2.50 BK a real po...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>According to the Finnish-Russian Chamber of Co...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Swedish buyout firm has sold its remaining...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5795</th>\n",
       "      <td>In 2009 , it reported net sales of approximate...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5796</th>\n",
       "      <td>H1 '08 H1 '07 Q2 '08 Q2 '07 in mln euro , unle...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5797</th>\n",
       "      <td>`` Low energy consumption and flexible loading...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5798</th>\n",
       "      <td>$SPY $MITK fast 56pc dive http://stks.co/3ffN $$</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5799</th>\n",
       "      <td>Investment management and investment advisory ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Sentence Sentiment\n",
       "0     The GeoSolutions technology will leverage Bene...  positive\n",
       "1     $ESI on lows, down $1.50 to $2.50 BK a real po...  negative\n",
       "2     For the last quarter of 2010 , Componenta 's n...  positive\n",
       "3     According to the Finnish-Russian Chamber of Co...   neutral\n",
       "4     The Swedish buyout firm has sold its remaining...   neutral\n",
       "...                                                 ...       ...\n",
       "5795  In 2009 , it reported net sales of approximate...   neutral\n",
       "5796  H1 '08 H1 '07 Q2 '08 Q2 '07 in mln euro , unle...   neutral\n",
       "5797  `` Low energy consumption and flexible loading...   neutral\n",
       "5798   $SPY $MITK fast 56pc dive http://stks.co/3ffN $$  negative\n",
       "5799  Investment management and investment advisory ...   neutral\n",
       "\n",
       "[5800 rows x 2 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('./train_data.csv') # 자신의 file path에 맞게 수정하시면 됩니다.\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e76e0a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_sentences(data):     \n",
    "    data = data.lower().split()\n",
    "        \n",
    "    stops = set(stopwords.words('english'))\n",
    "    data = [word for word in data if not word in stops] \n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    data = [lemmatizer.lemmatize(word) for word in data]\n",
    "\n",
    "    data = ' '.join(data)\n",
    "    data = re.sub(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",' ',data)   \n",
    "    data = re.sub('[^a-zA-z]',' ',data)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[3289, 56, 2596, 1149, 2, 1668, 39, 670, 618, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[4747, 2598, 3291, 137, 2599, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[77, 14, 529, 2, 11, 5, 1670, 1, 24, 1, 24, 17...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[62, 7, 295, 3292, 1895, 262, 88, 3, 16, 13, 1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[217, 1150, 358, 359, 833, 41, 141, 587, 4749,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Sentiment\n",
       "0  [3289, 56, 2596, 1149, 2, 1668, 39, 670, 618, ...          2\n",
       "1  [4747, 2598, 3291, 137, 2599, 0, 0, 0, 0, 0, 0...          0\n",
       "2  [77, 14, 529, 2, 11, 5, 1670, 1, 24, 1, 24, 17...          2\n",
       "3  [62, 7, 295, 3292, 1895, 262, 88, 3, 16, 13, 1...          1\n",
       "4  [217, 1150, 358, 359, 833, 41, 141, 587, 4749,...          1"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Sentence'] = train['Sentence'].apply(lambda x : cleaning_sentences(x))\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train['Sentiment'] = label_encoder.fit_transform(train['Sentiment'])\n",
    "\n",
    "all_words = ' '.join(train['Sentence']).split()\n",
    "freq_dist = FreqDist(all_words)\n",
    "\n",
    "sorted_word_freq = sorted(freq_dist.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "word_index = {word: index+1 for index, (word,_) in enumerate(sorted_word_freq)}\n",
    "\n",
    "train['Sentence'] = train['Sentence'].apply(lambda x : [word_index[word] for word in x.split()])\n",
    "\n",
    "# 제일 길이가 긴 문장을 기준으로 zero-padding 진행\n",
    "max_len = max(train['Sentence'].apply(len))\n",
    "train['Sentence'] = pad_sequences(train['Sentence'], maxlen=max_len, padding='post').tolist()\n",
    "\n",
    "# 최종 전처리된 데이터 확인\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ddb0a1",
   "metadata": {},
   "source": [
    "# Model\n",
    "전처리를 진행한 데이터를 이용하여 학습을 진행하는 부분입니다. 아래의 조건들에 맞는 코드를 작성하고, 결과를 확인하면 됩니다. 필요한만큼 셀을 사용하시면 됩니다.\n",
    "\n",
    "조건에 맞는 코드를 작성하고, 결과창(output)을 보여주어야합니다.\n",
    "\n",
    "- train data와 validation data 분리 → train : 8 / valid : 2 비율로 분리\n",
    "- target의 경우 classification이기 때문에 categorical하게 바꿔야 합니다.\n",
    "- MLP 모델 구현 → 하이퍼파라미터 설정은 자유\n",
    "- train / valid 과정 구현\n",
    "- train set loss/ validation set loss에 대한 learning curve 출력하기\n",
    "- 학습된 model은 학번_model.pth로 파일 save → ex) 20XXXXXXX_model.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4304c4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4640, 42]) torch.Size([4640, 3]) torch.Size([1160, 42]) torch.Size([1160, 3])\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# for reproducibility\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)\n",
    "    \n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train['Sentence'],train['Sentiment'],test_size=0.2,random_state=42)\n",
    "# train['Sentiment'] = to_categorical(train['Sentiment'], num_classes=3)\n",
    "y_train = to_categorical(y_train,num_classes=3)\n",
    "y_valid = to_categorical(y_valid,num_classes=3)\n",
    "\n",
    "X_train = torch.FloatTensor(X_train.tolist()).to(device)\n",
    "X_valid = torch.FloatTensor(X_valid.tolist()).to(device)\n",
    "y_train = torch.FloatTensor(y_train.tolist()).to(device)\n",
    "y_valid = torch.FloatTensor(y_valid.tolist()).to(device)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4640, 42])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(set(X_train.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.2141057252883911\n",
      "100 1.0053223371505737\n",
      "200 1.0089457035064697\n",
      "300 1.0141444206237793\n",
      "400 1.0151422023773193\n",
      "500 1.0122416019439697\n",
      "600 1.0119913816452026\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mf:\\GitHub\\code_learning_in_Uni\\3-2\\NLP\\HW3 감정분석 모델 만들기\\NLP_HW3_MLP.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/GitHub/code_learning_in_Uni/3-2/NLP/HW3%20%EA%B0%90%EC%A0%95%EB%B6%84%EC%84%9D%20%EB%AA%A8%EB%8D%B8%20%EB%A7%8C%EB%93%A4%EA%B8%B0/NLP_HW3_MLP.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/GitHub/code_learning_in_Uni/3-2/NLP/HW3%20%EA%B0%90%EC%A0%95%EB%B6%84%EC%84%9D%20%EB%AA%A8%EB%8D%B8%20%EB%A7%8C%EB%93%A4%EA%B8%B0/NLP_HW3_MLP.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# forward 연산\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/GitHub/code_learning_in_Uni/3-2/NLP/HW3%20%EA%B0%90%EC%A0%95%EB%B6%84%EC%84%9D%20%EB%AA%A8%EB%8D%B8%20%EB%A7%8C%EB%93%A4%EA%B8%B0/NLP_HW3_MLP.ipynb#X14sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m hypothesis \u001b[39m=\u001b[39m model(X_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/GitHub/code_learning_in_Uni/3-2/NLP/HW3%20%EA%B0%90%EC%A0%95%EB%B6%84%EC%84%9D%20%EB%AA%A8%EB%8D%B8%20%EB%A7%8C%EB%93%A4%EA%B8%B0/NLP_HW3_MLP.ipynb#X14sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# 비용 함수\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/GitHub/code_learning_in_Uni/3-2/NLP/HW3%20%EA%B0%90%EC%A0%95%EB%B6%84%EC%84%9D%20%EB%AA%A8%EB%8D%B8%20%EB%A7%8C%EB%93%A4%EA%B8%B0/NLP_HW3_MLP.ipynb#X14sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m cost \u001b[39m=\u001b[39m criterion(hypothesis, y_train)\n",
      "File \u001b[1;32mc:\\Users\\hyundong\\miniconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hyundong\\miniconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hyundong\\miniconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\hyundong\\miniconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hyundong\\miniconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hyundong\\miniconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "          nn.Linear(42, 10), # input_layer = 2, hidden_layer1 = 10\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(10, 10), # hidden_layer1 = 10, hidden_layer2 = 10\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(10, 10), # hidden_layer2 = 10, hidden_layer3 = 10\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(10, 3), # hidden_layer3 = 10, output_layer = 1\n",
    "          nn.Softmax(dim=1)  \n",
    "          ).to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=None, ignore_index=-100, reduction='mean').to(device) #다중 클래스 분류에 적합한 CrossEntropy loss를 위해서 soft max 사용\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "for epoch in range(10001):\n",
    "    optimizer.zero_grad()\n",
    "    # forward 연산\n",
    "    hypothesis = model(X_train)\n",
    "\n",
    "    # 비용 함수\n",
    "    cost = criterion(hypothesis, y_train)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100의 배수에 해당되는 에포크마다 비용을 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.224942684173584\n",
      "100 1.016479730606079\n",
      "200 1.0078165531158447\n",
      "300 0.9953939318656921\n",
      "400 0.9920290112495422\n",
      "500 0.9656710624694824\n",
      "600 0.9668888449668884\n",
      "700 0.976428210735321\n",
      "800 0.9636017084121704\n",
      "900 0.964198887348175\n",
      "1000 0.9519592523574829\n",
      "1100 0.9488985538482666\n",
      "1200 0.9664783477783203\n",
      "1300 0.943958580493927\n",
      "1400 0.9611387848854065\n",
      "1500 0.9481385350227356\n",
      "1600 0.956175684928894\n",
      "1700 1.013801097869873\n",
      "1800 1.0150519609451294\n",
      "1900 1.01504385471344\n",
      "2000 1.0150361061096191\n",
      "2100 1.0150288343429565\n",
      "2200 1.0150222778320312\n",
      "2300 1.015015959739685\n",
      "2400 1.015009880065918\n",
      "2500 1.015004277229309\n",
      "2600 1.0119874477386475\n",
      "2700 1.0115654468536377\n",
      "2800 1.0151594877243042\n",
      "2900 1.01515793800354\n",
      "3000 1.0151567459106445\n",
      "3100 1.0151554346084595\n",
      "3200 1.015154242515564\n",
      "3300 1.015153169631958\n",
      "3400 1.015151858329773\n",
      "3500 1.0151506662368774\n",
      "3600 1.0151493549346924\n",
      "3700 1.0151482820510864\n",
      "3800 1.015147089958191\n",
      "3900 1.0151457786560059\n",
      "4000 1.0151447057724\n",
      "4100 1.015143632888794\n",
      "4200 1.0151424407958984\n",
      "4300 1.0151413679122925\n",
      "4400 1.0151402950286865\n",
      "4500 1.0151389837265015\n",
      "4600 1.015138030052185\n",
      "4700 1.015136957168579\n",
      "4800 1.0151358842849731\n",
      "4900 1.0151348114013672\n",
      "5000 1.0151337385177612\n",
      "5100 1.0151326656341553\n",
      "5200 1.0151318311691284\n",
      "5300 1.015130877494812\n",
      "5400 1.015129804611206\n",
      "5500 1.0151289701461792\n",
      "5600 1.0151278972625732\n",
      "5700 1.0151268243789673\n",
      "5800 1.0151259899139404\n",
      "5900 1.0151251554489136\n",
      "6000 1.0151240825653076\n",
      "6100 1.0151232481002808\n",
      "6200 1.015122413635254\n",
      "6300 1.015121340751648\n",
      "6400 1.015120506286621\n",
      "6500 1.0151196718215942\n",
      "6600 1.0151188373565674\n",
      "6700 1.0151180028915405\n",
      "6800 1.0151171684265137\n",
      "6900 1.0151163339614868\n",
      "7000 1.01511549949646\n",
      "7100 1.015114665031433\n",
      "7200 1.0151139497756958\n",
      "7300 1.015113115310669\n",
      "7400 1.015112280845642\n",
      "7500 1.0151116847991943\n",
      "7600 1.0151108503341675\n",
      "7700 1.0151102542877197\n",
      "7800 1.0151093006134033\n",
      "7900 1.0151087045669556\n",
      "8000 1.0151078701019287\n",
      "8100 1.015107274055481\n",
      "8200 1.015106439590454\n",
      "8300 1.0151057243347168\n",
      "8400 1.015105128288269\n",
      "8500 1.0151045322418213\n",
      "8600 1.0151036977767944\n",
      "8700 1.0151031017303467\n",
      "8800 1.0151023864746094\n",
      "8900 1.0151017904281616\n",
      "9000 1.0151011943817139\n",
      "9100 1.0151004791259766\n",
      "9200 1.0150998830795288\n",
      "9300 1.015099287033081\n",
      "9400 1.0150985717773438\n",
      "9500 1.015098214149475\n",
      "9600 1.0150976181030273\n",
      "9700 1.01509690284729\n",
      "9800 1.0150963068008423\n",
      "9900 1.015095829963684\n",
      "10000 1.0150952339172363\n",
      "10100 1.0150946378707886\n",
      "10200 1.0150941610336304\n",
      "10300 1.0150935649871826\n",
      "10400 1.015093207359314\n",
      "10500 1.0150924921035767\n",
      "10600 1.015092134475708\n",
      "10700 1.0150914192199707\n",
      "10800 1.015091061592102\n",
      "10900 1.0150904655456543\n",
      "11000 1.015089988708496\n",
      "11100 1.0150896310806274\n",
      "11200 1.0150889158248901\n",
      "11300 1.0150885581970215\n",
      "11400 1.0150880813598633\n",
      "11500 1.0150874853134155\n",
      "11600 1.0150870084762573\n",
      "11700 1.0150866508483887\n",
      "11800 1.0150861740112305\n",
      "11900 1.0150856971740723\n",
      "12000 1.015085220336914\n",
      "12100 1.0150848627090454\n",
      "12200 1.0150842666625977\n",
      "12300 1.015083909034729\n",
      "12400 1.0150834321975708\n",
      "12500 1.0150830745697021\n",
      "12600 1.015082597732544\n",
      "12700 1.0150823593139648\n",
      "12800 1.0150818824768066\n",
      "12900 1.015081524848938\n",
      "13000 1.0150810480117798\n",
      "13100 1.0150805711746216\n",
      "13200 1.0150803327560425\n",
      "13300 1.0150798559188843\n",
      "13400 1.0150794982910156\n",
      "13500 1.0150792598724365\n",
      "13600 1.0150787830352783\n",
      "13700 1.0150784254074097\n",
      "13800 1.0150781869888306\n",
      "13900 1.015077829360962\n",
      "14000 1.0150773525238037\n",
      "14100 1.0150771141052246\n",
      "14200 1.015076756477356\n",
      "14300 1.0150762796401978\n",
      "14400 1.0150761604309082\n",
      "14500 1.01507568359375\n",
      "14600 1.0150752067565918\n",
      "14700 1.0150750875473022\n",
      "14800 1.015074610710144\n",
      "14900 1.015074372291565\n",
      "15000 1.0150740146636963\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "          nn.Linear(42, 30), # input_layer = 2, hidden_layer1 = 10\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(30, 20), # hidden_layer1 = 10, hidden_layer2 = 10\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(20, 10), # hidden_layer2 = 10, hidden_layer3 = 10\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(10, 3), # hidden_layer3 = 10, output_layer = 1\n",
    "          nn.Softmax(dim=1)  \n",
    "          ).to(device)\n",
    "\n",
    "learning_rate = 1\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=None, ignore_index=-100, reduction='mean').to(device) #다중 클래스 분류에 적합한 CrossEntropy loss를 위해서 soft max 사용\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for epoch in range(15001):\n",
    "    optimizer.zero_grad()\n",
    "    # forward 연산\n",
    "    hypothesis = model(X_train)\n",
    "\n",
    "    # 비용 함수\n",
    "    cost = criterion(hypothesis, y_train)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100의 배수에 해당되는 에포크마다 비용을 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델의 출력값(Hypothesis):  [[0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n",
      "모델의 예측값(Predicted):  [[0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n",
      "실제값(Y):  [[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "정확도(Accuracy):  0.6977011561393738\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    hypothesis = model(X_valid)\n",
    "    predicted = (hypothesis > 0.7).float()\n",
    "    accuracy = (predicted == y_valid).float().mean()\n",
    "    print('모델의 출력값(Hypothesis): ', hypothesis.detach().cpu().numpy())\n",
    "    print('모델의 예측값(Predicted): ', predicted.detach().cpu().numpy())\n",
    "    print('실제값(Y): ', y_valid.cpu().numpy())\n",
    "    print('정확도(Accuracy): ', accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Linear(42, 10, bias=True) 정확도 0.6876\n",
    "# nn.Linear(42, 10) 정확도 0.688\n",
    "# ReLU 사용 정확도 0.693\n",
    "# 층 1개 제거 0.6977\n",
    "\n",
    "\"\"\"\n",
    "model = nn.Sequential(\n",
    "          nn.Linear(42, 30), # input_layer = 2, hidden_layer1 = 10\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(30, 20), # hidden_layer1 = 10, hidden_layer2 = 10\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(20, 10), # hidden_layer2 = 10, hidden_layer3 = 10\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(10, 3), # hidden_layer3 = 10, output_layer = 1\n",
    "          nn.Softmax(dim=1)  \n",
    "          ).to(device)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46707b3",
   "metadata": {},
   "source": [
    "# 참고 자료\n",
    "\n",
    "- 아래 markdown cell에 내용 및 코드를 참고한 부분을 작성하시면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b1f6b2",
   "metadata": {},
   "source": [
    "딥러닝을 이용한 자연어 처리 입문 : https://wikidocs.net/49071\n",
    "PyTorch로 시작하는 딥 러닝 입문 : https://wikidocs.net/61010"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_test",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
