{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PAD]',\n",
       " '[UNK]',\n",
       " '[CLS]',\n",
       " '[SEP]',\n",
       " '[MASK]',\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '.',\n",
       " 'ㄱ',\n",
       " 'ㄲ',\n",
       " 'ㄳ',\n",
       " 'ㄴ',\n",
       " 'ㄵ',\n",
       " 'ㄶ',\n",
       " 'ㄷ',\n",
       " 'ㄸ',\n",
       " 'ㄹ',\n",
       " 'ㄺ',\n",
       " 'ㄻ',\n",
       " 'ㄼ',\n",
       " 'ㄽ',\n",
       " 'ㄾ',\n",
       " 'ㄿ',\n",
       " 'ㅀ',\n",
       " 'ㅁ',\n",
       " 'ㅂ',\n",
       " 'ㅃ',\n",
       " 'ㅄ',\n",
       " 'ㅅ',\n",
       " 'ㅆ',\n",
       " 'ㅇ',\n",
       " 'ㅈ',\n",
       " 'ㅉ',\n",
       " 'ㅊ',\n",
       " 'ㅋ',\n",
       " 'ㅌ',\n",
       " 'ㅍ',\n",
       " 'ㅎ',\n",
       " 'ㅏ',\n",
       " 'ㅐ',\n",
       " '##ㅇ',\n",
       " '##ㅈ',\n",
       " '##ㅐ',\n",
       " '##ㅅ',\n",
       " '##ㄴ',\n",
       " '##ㄷ',\n",
       " '##ㅁ',\n",
       " '##ㅀ',\n",
       " '##ㅏ',\n",
       " '##ㅍ',\n",
       " '##ㅊ',\n",
       " '##ㅎ',\n",
       " '##ㄹ',\n",
       " '##ㅂ',\n",
       " '##ㄱ',\n",
       " '##ㅌ',\n",
       " '##ㅋ',\n",
       " '##ㅆ',\n",
       " '##ㅉ',\n",
       " '##ㄲ',\n",
       " '##ㄸ',\n",
       " '##ㅃ',\n",
       " '##ㄺ',\n",
       " '##ㄳ',\n",
       " '##ㄾ',\n",
       " '##ㄵ',\n",
       " '##ㄻ',\n",
       " '##ㅄ',\n",
       " '##ㄶ',\n",
       " '##ㄼ',\n",
       " '##ㄽ',\n",
       " '##ㄿ',\n",
       " 'ㄷ자형',\n",
       " '[unused0]',\n",
       " '[unused1]',\n",
       " '[unused2]',\n",
       " '[unused3]',\n",
       " '[unused4]',\n",
       " '[unused5]',\n",
       " '[unused6]',\n",
       " '[unused7]',\n",
       " '[unused8]',\n",
       " '[unused9]',\n",
       " '[unused10]',\n",
       " '[unused11]',\n",
       " '[unused12]',\n",
       " '[unused13]',\n",
       " '[unused14]',\n",
       " '[unused15]',\n",
       " '[unused16]',\n",
       " '[unused17]',\n",
       " '[unused18]',\n",
       " '[unused19]',\n",
       " '[unused20]',\n",
       " '[unused21]',\n",
       " '[unused22]',\n",
       " '[unused23]',\n",
       " '[unused24]',\n",
       " '[unused25]',\n",
       " '[unused26]',\n",
       " '[unused27]',\n",
       " '[unused28]',\n",
       " '[unused29]',\n",
       " '[unused30]',\n",
       " '[unused31]',\n",
       " '[unused32]',\n",
       " '[unused33]',\n",
       " '[unused34]',\n",
       " '[unused35]',\n",
       " '[unused36]',\n",
       " '[unused37]',\n",
       " '[unused38]',\n",
       " '[unused39]',\n",
       " '[unused40]',\n",
       " '[unused41]',\n",
       " '[unused42]',\n",
       " '[unused43]',\n",
       " '[unused44]',\n",
       " '[unused45]',\n",
       " '[unused46]',\n",
       " '[unused47]',\n",
       " '[unused48]',\n",
       " '[unused49]',\n",
       " '[unused50]',\n",
       " '[unused51]',\n",
       " '[unused52]',\n",
       " '[unused53]',\n",
       " '[unused54]',\n",
       " '[unused55]',\n",
       " '[unused56]',\n",
       " '[unused57]',\n",
       " '[unused58]',\n",
       " '[unused59]',\n",
       " '[unused60]',\n",
       " '[unused61]',\n",
       " '[unused62]',\n",
       " '[unused63]',\n",
       " '[unused64]',\n",
       " '[unused65]',\n",
       " '[unused66]',\n",
       " '[unused67]',\n",
       " '[unused68]',\n",
       " '[unused69]',\n",
       " '[unused70]',\n",
       " '[unused71]',\n",
       " '[unused72]',\n",
       " '[unused73]',\n",
       " '[unused74]',\n",
       " '[unused75]',\n",
       " '[unused76]',\n",
       " '[unused77]',\n",
       " '[unused78]',\n",
       " '[unused79]',\n",
       " '[unused80]',\n",
       " '[unused81]',\n",
       " '[unused82]',\n",
       " '[unused83]',\n",
       " '[unused84]',\n",
       " '[unused85]',\n",
       " '[unused86]',\n",
       " '[unused87]',\n",
       " '[unused88]',\n",
       " '[unused89]',\n",
       " '[unused90]',\n",
       " '[unused91]',\n",
       " '[unused92]',\n",
       " '[unused93]',\n",
       " '[unused94]',\n",
       " '[unused95]',\n",
       " '[unused96]',\n",
       " '[unused97]',\n",
       " '[unused98]',\n",
       " '[unused99]',\n",
       " '[unused100]',\n",
       " '[unused101]',\n",
       " '[unused102]',\n",
       " '[unused103]',\n",
       " '[unused104]',\n",
       " '[unused105]',\n",
       " '[unused106]',\n",
       " '[unused107]',\n",
       " '[unused108]',\n",
       " '[unused109]',\n",
       " '[unused110]',\n",
       " '[unused111]',\n",
       " '[unused112]',\n",
       " '[unused113]',\n",
       " '[unused114]',\n",
       " '[unused115]',\n",
       " '[unused116]',\n",
       " '[unused117]',\n",
       " '[unused118]',\n",
       " '[unused119]',\n",
       " '[unused120]',\n",
       " '[unused121]',\n",
       " '[unused122]',\n",
       " '[unused123]',\n",
       " '[unused124]',\n",
       " '[unused125]',\n",
       " '[unused126]',\n",
       " '[unused127]',\n",
       " '[unused128]',\n",
       " '[unused129]',\n",
       " '[unused130]',\n",
       " '[unused131]',\n",
       " '[unused132]',\n",
       " '[unused133]',\n",
       " '[unused134]',\n",
       " '[unused135]',\n",
       " '[unused136]',\n",
       " '[unused137]',\n",
       " '[unused138]',\n",
       " '[unused139]',\n",
       " '[unused140]',\n",
       " '[unused141]',\n",
       " '[unused142]',\n",
       " '[unused143]',\n",
       " '[unused144]',\n",
       " '[unused145]',\n",
       " '[unused146]',\n",
       " '[unused147]',\n",
       " '[unused148]',\n",
       " '[unused149]',\n",
       " '[unused150]',\n",
       " '[unused151]',\n",
       " '[unused152]',\n",
       " '[unused153]',\n",
       " '[unused154]',\n",
       " '[unused155]',\n",
       " '[unused156]',\n",
       " '[unused157]',\n",
       " '[unused158]',\n",
       " '[unused159]',\n",
       " '[unused160]',\n",
       " '[unused161]',\n",
       " '[unused162]',\n",
       " '[unused163]',\n",
       " '[unused164]',\n",
       " '[unused165]',\n",
       " '[unused166]',\n",
       " '[unused167]',\n",
       " '[unused168]',\n",
       " '[unused169]',\n",
       " '[unused170]',\n",
       " '[unused171]',\n",
       " '[unused172]',\n",
       " '[unused173]',\n",
       " '[unused174]',\n",
       " '[unused175]',\n",
       " '[unused176]',\n",
       " '[unused177]',\n",
       " '[unused178]',\n",
       " '[unused179]',\n",
       " '[unused180]',\n",
       " '[unused181]',\n",
       " '[unused182]',\n",
       " '[unused183]',\n",
       " '[unused184]',\n",
       " '[unused185]',\n",
       " '[unused186]',\n",
       " '[unused187]',\n",
       " '[unused188]',\n",
       " '[unused189]',\n",
       " '[unused190]',\n",
       " '[unused191]',\n",
       " '[unused192]',\n",
       " '[unused193]',\n",
       " '[unused194]',\n",
       " '[unused195]',\n",
       " '[unused196]',\n",
       " '[unused197]',\n",
       " '[unused198]',\n",
       " '[unused199]']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab.txt에 특수 기호가 있는지 확인\n",
    "\n",
    "import re\n",
    "\n",
    "# 예시 파일 이름\n",
    "example_file_name = \"/Users/joon/Library/CloudStorage/GoogleDrive-j3837301@gmail.com/내 드라이브/자연어처리/baseline/KIPIKorPatELECTRA/vocab.txt\"\n",
    "\n",
    "# '#' 문자를 제외한 특수 문자가 있는 단어를 찾는 함수\n",
    "def find_words_with_special_characters_excluding_hash(file_path):\n",
    "    special_character_words = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            word = line.strip()\n",
    "            if re.search(r\"[^a-zA-Z0-9가-힣#]\", word):\n",
    "                special_character_words.append(word)\n",
    "    return special_character_words\n",
    "\n",
    "# '#'을 제외한 특수 문자가 포함된 단어 찾기\n",
    "words_with_special_characters_excluding_hash = find_words_with_special_characters_excluding_hash(example_file_name)\n",
    "words_with_special_characters_excluding_hash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Β', '㎬', '″', 'ㅕ', '㉸', 'Ω', 'ㅤ', '∩', '～', '◇', '┻', 'ｓ', '⑷', '⊃', 'ß', 'ㅀ', 'ㅹ', '{', 'ｒ', 'ㅔ', 'Φ', '☎', '&', '˝', '┍', '㉹', '!', 'ㅃ', '≫', 'Ｏ', 'ε', '＃', 'Ｍ', '」', '》', '㎎', '?', '∪', 'ｄ', '≠', 'ㅠ', 'Ω', 'Ｐ', '㎓', '“', 'ⅱ', '⑭', '¾', 'π', '＂', '㎘', 'Ｙ', '≪', 'Ｔ', '⇒', 'Ｌ', '㎃', '┌', 'ㅝ', '′', '○', '㎪', '＼', '_', '´', '─', '┳', '∈', 'θ', '┤', '*', '⒞', '㉡', '㏖', '㎣', '⒢', 'ㅡ', 'ｉ', '㎳', 'ｕ', '《', 'Ⅴ', '™', '┫', 'ⓕ', 'ｙ', 'Ｅ', '［', 'ㅅ', 'Ⅵ', 'ｐ', '┓', '１', '７', 'ℓ', 'ζ', 'у', 'ㄹ', '”', '㎐', '┖', '⑪', '⑧', '\\\\', 'ⅹ', '/', 'ㅍ', '△', '㎟', 'ⅴ', '┗', 'Ｇ', '｝', '８', '∃', 'ｂ', '⑮', '②', 'Ｕ', '├', '⑨', '�', 'Ｘ', '√', '。', '(', '｜', 'Ｆ', 'ㅒ', '）', '÷', '◎', '㎝', '⒟', 'μ', 'φ', 'Ｎ', '㎖', '㎉', 'ㅞ', '￢', ';', '∞', '㎽', 'τ', 'ｆ', '㉵', '‥', '┚', '㉶', '＆', '℃', '⑫', 'ㅐ', 'ㅂ', '＝', 'ㄸ', '┏', '｛', '∧', '㎂', 'ㅧ', 'ⓔ', 'ㅖ', '#', 'ｍ', 'ｔ', '⒠', '↔', '６', '㎢', 'ｅ', '≤', 'ㄱ', '※', '㎤', '㎩', '=', 'ㅘ', '-', '㎸', 'Ⅹ', '@', '‰', '▼', 'Ｉ', '╋', '.', 'ㅇ', \"'\", '＠', '`', 'ㅋ', '―', 'ㅿ', '（', 'ψ', '０', '│', 'Ｈ', 'ⅷ', 'ⅸ', 'ㅢ', '㎡', 'ｌ', '∼', '㉯', '∠', '㎒', '●', 'Ø', '５', '㎷', '：', '∽', 'ⓗ', '<', 'ㅚ', '≡', '┐', 'ω', '¼', 'Р', 'ㅛ', 'ㅉ', '③', 'Ⅰ', '㎌', 'ㅁ', 'ⓖ', '＜', 'Ⅶ', '≒', '㉮', 'ｗ', '└', 'ⅳ', '¸', '₃', '>', '℉', 'Ｒ', '+', '＞', 'ㄽ', '』', 'Ｚ', 'ㅗ', '≥', 'Ｓ', '㎥', 'Ｖ', 'ㅎ', 'Μ', '⑦', 'λ', '【', '３', 'ρ', ']', '·', 'Ｃ', 'ⅵ', 'ⓑ', '㉣', 'Х', '∫', '⑸', 'ⅲ', '４', '㎗', '㎾', 'Γ', 'Ⅷ', '|', 'ㅜ', '♀', '／', 'ㄾ', 'Ⅸ', '「', '∨', '╉', '〉', '½', 'ｖ', '₂', '㉷', '⑤', 'Ｗ', 'ㄴ', '㎞', '①', '㎚', '☆', 'ㆍ', '^', '㉱', '￠', '㉳', '♂', 'ｇ', '＋', '°', '%', '〕', 'ㄲ', 'Å', 'Ⅳ', 'Ф', 'ν', '▶', 'Τ', '⁴', 'Ｑ', 'ㄺ', '}', 'ｋ', 'Δ', 'ㄳ', '㉰', '‘', 'ㄷ', 'ｎ', 'ⅰ', '＇', '㎜', 'Π', '㎛', '¤', '㎍', 'ｈ', 'χ', '９', '⒜', 'κ', '㎋', '〈', '⑶', 'ξ', '⊙', 'Ｋ', '，', '↑', '…', '⑩', '▽', 'ㅄ', 'П', '㏄', 'ⅶ', '㎧', '→', 'ㆇ', '⊆', '㎠', '㎑', '↓', '┬', '┣', 'ㅓ', 'ａ', '㉢', 'ⓐ', '━', '†', '＊', '┼', '∥', '⊂', 'ㄿ', 'α', 'ㄻ', 'ⓒ', '⑥', '『', 'ｘ', '²', '’', '±', 'Ⅱ', '－', '％', '㉠', 'υ', '㉺', 'η', '[', 'Ｄ', 'ι', '㎏', 'С', '④', '®', ':', '∑', '．', 'ㅏ', 'Ｊ', '、', '⅓', '；', 'Σ', '㎕', 'ø', 'ㄼ', '⑴', 'ㅙ', '┘', 'Ν', '□', '⑬', '┛', '×', 'ⓚ', 'ㅌ', 'σ', ')', '２', 'Ⅲ', 'ｏ', '³', '〔', 'Θ', 'ㅈ', '］', '$', 'ⓓ', '₁', 'Ａ', 'Ｂ', 'δ', '⒡', '㎫', 'ㅟ', 'ㅊ', 'ㅣ', '〓', '⊥', 'ㅑ', 'ㅆ', '⒝', '~', 'β', '\\xad', 'γ', ',', '⑵', '■', 'ｃ', '˚'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_csv('./baseline/train.csv')\n",
    "claim_df = df[~df['claims'].isna()]\n",
    "\n",
    "# 특수문자를 찾는 함수\n",
    "def find_special_characters(series):\n",
    "    # 문자열에서 특수문자를 찾는 함수\n",
    "    special_chars = set()\n",
    "    for item in series:\n",
    "        for char in item:\n",
    "            if (not char.isalnum() or bool(re.search(r'[^a-zA-Z0-9가-힣]', char))) and not char.isspace():\n",
    "                # 한자 제외\n",
    "                if ord(char) in range(0x4E00, 0x9FFF + 1):\n",
    "                    continue\n",
    "                # 한자-확장A 제외\n",
    "                if ord(char) in range(0x3400, 0x4DBF + 1):\n",
    "                    continue\n",
    "                # 한자-확장B 제외\n",
    "                if ord(char) in range(0x20000, 0x2A6DF + 1):\n",
    "                    continue\n",
    "                # 호환 한자 제외\n",
    "                if ord(char) in range(0xF900, 0xFAFF + 1):\n",
    "                    continue\n",
    "                # 중국어 제외\n",
    "                if ord(char) in range(0xAC00, 0xD7AF + 1):\n",
    "                    continue\n",
    "                # 히라가나 제외\n",
    "                if ord(char) in range(0x3040, 0x309F + 1):\n",
    "                    continue\n",
    "                # 가타카나 제외\n",
    "                if ord(char) in range(0x30A0, 0x30FF + 1):\n",
    "                    continue\n",
    "                special_chars.add(char)\n",
    "    return special_chars\n",
    "\n",
    "# 'string' 컬럼에 대해 특수문자 찾기\n",
    "\n",
    "special_characters_1 = find_special_characters(df['invention_title'])\n",
    "special_characters_2 = find_special_characters(df['abstract'])\n",
    "special_characters_3 = find_special_characters(claim_df['claims'])\n",
    "print(special_characters_1 | special_characters_2 | special_characters_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 특수 기호를 찾는 함수\n",
    "def find_specific_characters(df, column, characters):\n",
    "    indices = []\n",
    "    ls = []\n",
    "    for index, text in df[column].items():\n",
    "        found_chars = [char for char in characters if char in text]\n",
    "        for char in found_chars:\n",
    "            if char in ls:\n",
    "                continue\n",
    "            indices.append(index)\n",
    "            ls.append(char)\n",
    "    return indices, ls\n",
    "\n",
    "# 'string' 컬럼에서 특정 특수 문자 찾기\n",
    "found_indices, found_special = find_specific_characters(df, 'invention_title', special_characters_1)\n",
    "df_invention_title = df.loc[found_indices, ['documentId', 'invention_title']]\n",
    "df_invention_title['special'] = found_special\n",
    "df_invention_title = df_invention_title[['special', 'invention_title']]\n",
    "df_invention_title.to_csv('./special_invention_title.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'string' 컬럼에서 특정 특수 문자 찾기\n",
    "found_indices, found_special = find_specific_characters(df, 'abstract', special_characters_2)\n",
    "df_abstract = df.loc[found_indices, ['documentId', 'abstract']]\n",
    "df_abstract['special'] = found_special\n",
    "df_abstract = df_abstract[['special', 'abstract']]\n",
    "df_abstract.to_csv('./special_abstract.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_df = df[~df['claims'].isna()]\n",
    "# 'string' 컬럼에서 특정 특수 문자 찾기\n",
    "found_indices, found_special = find_specific_characters(claims_df, 'claims', special_characters_2)\n",
    "df_claims = df.loc[found_indices, ['documentId', 'claims']]\n",
    "df_claims['special'] = found_special\n",
    "df_claims = df_claims[['special', 'claims']]\n",
    "df_claims.to_csv('./special_claims.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
