{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "learning_rate = 0.0002\n",
    "num_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = datasets.MNIST(root=\"../Data/\",train=True,transform = transforms.ToTensor(),target_transform=None, download=True)\n",
    "mnist_test = datasets.MNIST(root=\"../Data/\",train=False,transform = transforms.ToTensor(),target_transform=None, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(mnist_train,batch_size=batch_size,shuffle=True,num_workers=2,drop_last=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size,shuffle=False,num_workers=2,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,out_channels=16,kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=16,out_channels=32,kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        )   \n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(64*3*3,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,10)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.layer(x)\n",
    "        out = out.view(batch_size,-1)\n",
    "        out = self.fc_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3089, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1180, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1222, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0715, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0139, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0322, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0324, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0086, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0324, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0388, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss_arr = []\n",
    "for i in range(num_epoch):\n",
    "    for j,[image,label] in enumerate(train_loader):\n",
    "        x = image.to(device)\n",
    "        y = label.to(device)\n",
    "\n",
    "        optimizer.zero_grad() # pytorch 특성 : back propagation할때마다 gradient 누적.\n",
    "                              # 따라서 각 배치에서 그래디언트를 계산하기 전에 초기화\n",
    "        \n",
    "        output = model.forward(x) # 모델에 x를 전달하고 출력(y_label) 반환.\n",
    "\n",
    "        loss = loss_func(output,y) # loss 계산\n",
    "        loss.backward() # loss에 대한 gradient 계산\n",
    "        optimizer.step() # 모델 가중치 업데이트\n",
    "\n",
    "        if j % 1000 == 0:\n",
    "            print(loss)\n",
    "            loss_arr.append(loss.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Test Data: 99.16999816894531%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# 해당 모델의 모든 레이어가 eval mode에 들어가게 한다. \n",
    "# -? 학습할 때만 사용하는 개념인 Dropout이나 Batchnorm 등을 비활성화 시킨다는 것을 의미.\n",
    "model.eval()\n",
    "\n",
    "# torch's autograd engine 비활성화. gradient 트래킹을 더이상 하지 않음. \n",
    "with torch.no_grad():\n",
    "    for image, label in test_loader:\n",
    "        x = image.to(device)\n",
    "        y = label.to(device)\n",
    "\n",
    "        output = model.forward(x)\n",
    "\n",
    "        _,output_index = torch.max(output,1)\n",
    "\n",
    "        total += label.size(0)\n",
    "\n",
    "        correct += (output_index == y).sum().float()\n",
    "\n",
    "    print(\"Accuracy of Test Data: {}%\".format(100*correct/total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
